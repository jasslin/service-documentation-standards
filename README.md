# Production Service Documentation Standards
# ç”Ÿç”¢æœå‹™æ–‡ä»¶æ¨™æº–

## Technical Framework for Service Resilience
## æœå‹™éŸŒæ€§æŠ€è¡“æ¡†æ¶

---

> **âš ï¸ PURPOSE ç›®çš„**
>
> **This framework provides automated checks and best practices to prevent service outages.**  
> **æ­¤æ¡†æ¶æä¾›è‡ªå‹•åŒ–æª¢æŸ¥å’Œæœ€ä½³å¯¦è¸ä»¥é˜²æ­¢æœå‹™ä¸­æ–·ã€‚**
>
> **Two levels of requirements:**
> - ğŸ”´ **Hard Gates**: Automated checks that block merge/release
> - ğŸŸ¡ **Aspirational**: Recommended practices that improve quality
>
> **å…©ç´šè¦æ±‚ï¼š**
> - ğŸ”´ **Hard Gatesï¼ˆç¡¬æ€§é–˜é–€ï¼‰**ï¼šé˜»æ­¢ merge/release çš„è‡ªå‹•åŒ–æª¢æŸ¥
> - ğŸŸ¡ **Aspirationalï¼ˆå»ºè­°æ¨™æº–ï¼‰**ï¼šæå‡å“è³ªçš„å»ºè­°å¯¦è¸

---

## Table of Contents (ç›®éŒ„)

1. [The Incident: Why This Framework Exists](#the-incident-why-this-framework-exists)
2. [Framework Purpose](#framework-purpose)
3. [Core Standards](#core-standards)
4. [Definition of Done](#definition-of-done)
5. [Documentation Structure](#documentation-structure)
6. [Enforcement & Compliance](#enforcement-and-compliance)
7. [Authorization](#authorization)

---

## The Incident: Why This Framework Exists
## äº‹æ•…èƒŒæ™¯ï¼šç‚ºä½•éœ€è¦æ­¤æ¡†æ¶

### THE INCIDENT (äº‹æ•…)

**Date**: January 2026  
**æ—¥æœŸ**ï¼š2026å¹´1æœˆ

**Classification**: Critical Service Outage â€” Complete System Failure  
**åˆ†é¡**ï¼šé—œéµæœå‹™ä¸­æ–· â€” ç³»çµ±å®Œå…¨æ•…éšœ

**Total Downtime**: **TWO WEEKS** of complete service unavailability  
**ç¸½åœæ©Ÿæ™‚é–“**ï¼š**å…©é€±**çš„å®Œå…¨æœå‹™ä¸å¯ç”¨

---

### What Happened (ç™¼ç”Ÿäº†ä»€éº¼)

A routine server reboot was performed on a production system hosting the **Flemabus** service for a mission-critical client.  
å°è¨—ç®¡ **Flemabus** æœå‹™çš„ç”Ÿç”¢ä¼ºæœå™¨åŸ·è¡Œä¾‹è¡Œæ€§é‡å•Ÿï¼Œè©²ç³»çµ±ç‚ºé—œéµå®¢æˆ¶éƒ¨ç½²ã€‚

**The server came back online. The services did not.**  
**ä¼ºæœå™¨é‡æ–°ä¸Šç·šäº†ã€‚ä½†æœå‹™æ²’æœ‰ã€‚**

Multiple engineers investigated, they found:  
å¤šåå·¥ç¨‹å¸«èª¿æŸ¥æ™‚ç™¼ç¾ï¼š

```bash
docker ps
# Result: Empty. No containers running.
# çµæœï¼šç©ºç™½ã€‚æ²’æœ‰å®¹å™¨åœ¨é‹è¡Œã€‚

systemctl status docker.service
# Output: inactive (dead)
# è¼¸å‡ºï¼šä¸æ´»èºï¼ˆå·²åœæ­¢ï¼‰
```

**But they could not solve the problem.**  
**ä½†ä»–å€‘ç„¡æ³•è§£æ±ºå•é¡Œã€‚**

Days passed. Engineers attempted various recovery approaches. All failed. The system configuration was undocumented. Environment variables were unknown. The architecture existed only in scattered chat logs and vendor memory.  
æ•¸å¤©éå»äº†ã€‚å·¥ç¨‹å¸«å˜—è©¦äº†å„ç¨®æ¢å¾©æ–¹æ³•ã€‚å…¨éƒ¨å¤±æ•—ã€‚ç³»çµ±é…ç½®æœªè¨˜éŒ„ã€‚ç’°å¢ƒè®Šæ•¸æœªçŸ¥ã€‚æ¶æ§‹åƒ…å­˜åœ¨æ–¼é›¶æ•£çš„èŠå¤©è¨˜éŒ„å’Œå» å•†è¨˜æ†¶ä¸­ã€‚

**It was not until a specific engineer (Ezra Wu) was consulted that the problem could be diagnosed and resolved.**  
**ç›´åˆ°è«®è©¢ç‰¹å®šå·¥ç¨‹å¸«ï¼ˆå³å­éƒ‡ï¼‰å¾Œï¼Œå•é¡Œæ‰å¾—ä»¥è¨ºæ–·å’Œè§£æ±ºã€‚**

This exposed a **critical single point of failure**: **Critical system knowledge existed only in one person's memory.**  
é€™æš´éœ²äº†ä¸€å€‹**é—œéµå–®é»æ•…éšœ**ï¼š**é—œéµç³»çµ±çŸ¥è­˜åƒ…å­˜åœ¨æ–¼ä¸€å€‹äººçš„è¨˜æ†¶ä¸­ã€‚**

**The recovery process took TWO WEEKS** â€” not because of technical complexity, but because of **complete absence of documentation**.  
**æ¢å¾©éç¨‹è€—æ™‚å…©é€±** â€” ä¸æ˜¯å› ç‚ºæŠ€è¡“è¤‡é›œæ€§ï¼Œè€Œæ˜¯å› ç‚º**å®Œå…¨ç¼ºä¹æ–‡ä»¶è¨˜éŒ„**ã€‚

**Two weeks of zero service availability. Two weeks of client business interruption. Two weeks of reputational damage.**  
**å…©é€±çš„é›¶æœå‹™å¯ç”¨æ€§ã€‚å…©é€±çš„å®¢æˆ¶æ¥­å‹™ä¸­æ–·ã€‚å…©é€±çš„è²è­½æå®³ã€‚**

---

### Root Cause Analysis (æ ¹æœ¬åŸå› åˆ†æ)

The Post-Mortem investigation revealed **multiple critical failures**:  
äº‹å¾Œæª¢è¨èª¿æŸ¥æ­éœ²äº†**å¤šå€‹é—œéµå¤±èª¤**ï¼š

#### Failure #1: Lack of Environment Isolation (ç¼ºä¹ç’°å¢ƒéš”é›¢)

**Finding**: New deployment attempted to use the same Docker network name as the existing production system, causing network conflicts that broke both systems.  
**ç™¼ç¾**ï¼šæ–°éƒ¨ç½²å˜—è©¦ä½¿ç”¨èˆ‡ç¾æœ‰ç”Ÿç”¢ç³»çµ±ç›¸åŒçš„ Docker network åç¨±ï¼Œå°è‡´ç¶²è·¯è¡çªä½¿å…©å€‹ç³»çµ±éƒ½æå£ã€‚

```bash
# Both deployments tried to create:
docker network create app-network
# Error: network with name app-network already exists

# Result: New deployment failed AND existing system network corrupted
# çµæœï¼šæ–°éƒ¨ç½²å¤±æ•—ä¸”ç¾æœ‰ç³»çµ±ç¶²è·¯æå£
```

**What this means**: Without proper naming conventions and environment isolation, deployments can interfere with each other.  
**é€™æ„å‘³è‘—ä»€éº¼**ï¼šæ²’æœ‰é©ç•¶çš„å‘½åè¦ç¯„å’Œç’°å¢ƒéš”é›¢ï¼Œéƒ¨ç½²æœƒç›¸äº’å¹²æ“¾ã€‚

**Professional Standard**: Each deployment must use uniquely named resources (networks, volumes, container names).  
**å°ˆæ¥­æ¨™æº–**ï¼šæ¯å€‹éƒ¨ç½²å¿…é ˆä½¿ç”¨å”¯ä¸€å‘½åçš„è³‡æºï¼ˆç¶²è·¯ã€å·ã€å®¹å™¨åç¨±ï¼‰ã€‚

#### Failure #2: Manual `docker-compose down` Without Understanding Impact (æ‰‹å‹•åŸ·è¡Œ docker-compose down è€Œä¸ç†è§£å½±éŸ¿)

**Finding**: Engineer manually executed `docker-compose down` in the wrong directory, bringing down the production system instead of the test deployment.  
**ç™¼ç¾**ï¼šå·¥ç¨‹å¸«åœ¨éŒ¯èª¤çš„ç›®éŒ„ä¸­æ‰‹å‹•åŸ·è¡Œ `docker-compose down`ï¼Œå°è‡´ç”Ÿç”¢ç³»çµ±è€Œéæ¸¬è©¦éƒ¨ç½²åœæ­¢ã€‚

```bash
# Engineer thought they were in test directory
cd /opt/test-deployment  # Actually still in /opt/production
docker-compose down      # âŒ Brought down PRODUCTION

# Without network names in compose files, couldn't easily identify which was which
# æ²’æœ‰åœ¨ compose æª”æ¡ˆä¸­ä½¿ç”¨ç¶²è·¯åç¨±ï¼Œç„¡æ³•è¼•æ˜“è­˜åˆ¥å“ªå€‹æ˜¯å“ªå€‹
```

**What this means**: Manual operations without clear naming and safeguards lead to catastrophic mistakes.  
**é€™æ„å‘³è‘—ä»€éº¼**ï¼šæ²’æœ‰æ˜ç¢ºå‘½åå’Œå®‰å…¨æªæ–½çš„æ‰‹å‹•æ“ä½œå°è‡´ç½é›£æ€§éŒ¯èª¤ã€‚

**Professional Standard**: Production systems must have clear identifiers and require explicit confirmation before destructive operations.  
**å°ˆæ¥­æ¨™æº–**ï¼šç”Ÿç”¢ç³»çµ±å¿…é ˆæœ‰æ¸…æ™°çš„è­˜åˆ¥ç¬¦ï¼Œåœ¨ç ´å£æ€§æ“ä½œå‰éœ€è¦æ˜ç¢ºç¢ºèªã€‚

#### Failure #3: No Rollback Plan or Backup Configuration (ç„¡å›æ»¾è¨ˆåŠƒæˆ–å‚™ä»½é…ç½®)

**Finding**: When the production system went down, there was no documented way to restore it.  
**ç™¼ç¾**ï¼šç•¶ç”Ÿç”¢ç³»çµ±åœæ­¢æ™‚ï¼Œæ²’æœ‰è¨˜éŒ„çš„æ¢å¾©æ–¹æ³•ã€‚

- No backup of docker-compose.yml
- No backup of .env file
- No documentation of network configuration
- No record of which containers were running

**What this means**: System recovery took TWO WEEKS because everything had to be reconstructed from memory.  
**é€™æ„å‘³è‘—ä»€éº¼**ï¼šç³»çµ±æ¢å¾©èŠ±äº†å…©é€±æ™‚é–“ï¼Œå› ç‚ºä¸€åˆ‡éƒ½å¿…é ˆå¾è¨˜æ†¶ä¸­é‡å»ºã€‚

#### Failure #4: Service Persistence Not Configured (æœå‹™æŒä¹…æ€§æœªé…ç½®)

**Finding**: Even after reconstructing the configuration, services didn't survive a simple reboot because:  
**ç™¼ç¾**ï¼šå³ä½¿é‡å»ºé…ç½®å¾Œï¼Œæœå‹™ä¹Ÿç„¡æ³•ç¶“å¾—èµ·ç°¡å–®çš„é‡å•Ÿï¼Œå› ç‚ºï¼š

- Docker daemon not enabled for auto-start
- No `restart: always` in container configuration

```bash
systemctl is-enabled docker.service
# Actual: disabled âŒ
```

**What this means**: Any reboot requires manual intervention.  
**é€™æ„å‘³è‘—ä»€éº¼**ï¼šä»»ä½•é‡å•Ÿéƒ½éœ€è¦äººå·¥ä»‹å…¥ã€‚

---

### The Compounding Factors (è¤‡åˆå› ç´ )

Beyond the two primary failures, the incident exposed systemic weaknesses:  
é™¤äº†å…©å€‹ä¸»è¦å¤±èª¤å¤–ï¼Œè©²äº‹æ•…é‚„æš´éœ²äº†ç³»çµ±æ€§å¼±é»ï¼š

#### No Documentation (ç„¡æ–‡ä»¶è¨˜éŒ„)

There was **no written deployment guide**. The "knowledge" of how to deploy and maintain the system existed only in:  
**æ²’æœ‰æ›¸é¢éƒ¨ç½²æŒ‡å—**ã€‚å¦‚ä½•éƒ¨ç½²å’Œç¶­è­·ç³»çµ±çš„ã€ŒçŸ¥è­˜ã€åƒ…å­˜åœ¨æ–¼ï¼š

- The vendor's memory (å» å•†è¨˜æ†¶)
- Undocumented chat messages (æœªè¨˜éŒ„çš„èŠå¤©è¨Šæ¯)  
- Assumptions and tribal knowledge (å‡è¨­å’Œéƒ¨è½çŸ¥è­˜)
- **One specific engineer's personal memory (ç‰¹å®šå·¥ç¨‹å¸«çš„å€‹äººè¨˜æ†¶)**

**This created a critical single point of failure: Only ONE person could recover the system.**  
**é€™é€ æˆäº†é—œéµå–®é»æ•…éšœï¼šåªæœ‰ä¸€å€‹äººèƒ½å¤ æ¢å¾©ç³»çµ±ã€‚**

When multiple engineers attempted recovery and failed, the client suffered **continuous downtime** until that specific engineer could be reached. **This is unacceptable.**  
ç•¶å¤šåå·¥ç¨‹å¸«å˜—è©¦æ¢å¾©ä½†å¤±æ•—æ™‚ï¼Œå®¢æˆ¶é­å—**æŒçºŒåœæ©Ÿ**ï¼Œç›´åˆ°è©²ç‰¹å®šå·¥ç¨‹å¸«å¯ä»¥è¯ç¹«ä¸Šã€‚**é€™æ˜¯ä¸å¯æ¥å—çš„ã€‚**

**The result: It took TWO WEEKS to reconstruct the system from scratch.**  
**çµæœï¼šå¾é›¶é–‹å§‹é‡å»ºç³»çµ±èŠ±äº†å…©é€±æ™‚é–“ã€‚**

Engineers had to reverse-engineer the entire configuration, guess at environment variables, and piece together the architecture through trial and error â€” because **no documentation existed**.  
å·¥ç¨‹å¸«å¿…é ˆå°æ•´å€‹é…ç½®é€²è¡Œé€†å‘å·¥ç¨‹ï¼ŒçŒœæ¸¬ç’°å¢ƒè®Šæ•¸ï¼Œä¸¦é€éè©¦éŒ¯æ³•æ‹¼æ¹Šæ¶æ§‹ â€” å› ç‚º**æ²’æœ‰æ–‡ä»¶è¨˜éŒ„å­˜åœ¨**ã€‚

#### No Staging Verification (ç„¡é ç™¼ç’°å¢ƒé©—è­‰)

The system was deployed directly to production **without any resilience testing**. Specifically:  
ç³»çµ±ç›´æ¥éƒ¨ç½²åˆ°ç”Ÿç”¢ç’°å¢ƒï¼Œ**æœªç¶“ä»»ä½•éŸŒæ€§æ¸¬è©¦**ã€‚å…·é«”è€Œè¨€ï¼š

- âŒ No hard reboot test was performed  
  âŒ æœªåŸ·è¡Œç¡¬é‡å•Ÿæ¸¬è©¦

- âŒ No automated recovery verification  
  âŒ ç„¡è‡ªå‹•æ¢å¾©é©—è­‰

- âŒ No acceptance criteria checklist  
  âŒ ç„¡é©—æ”¶æ¨™æº–æª¢æŸ¥æ¸…å–®

**The first time the system's resilience was tested was during a production reboot. It failed. The client suffered TWO WEEKS of downtime.**  
**ç³»çµ±éŸŒæ€§é¦–æ¬¡æ¸¬è©¦æ˜¯åœ¨ç”Ÿç”¢ç’°å¢ƒé‡å•ŸæœŸé–“ã€‚å®ƒå¤±æ•—äº†ã€‚å®¢æˆ¶é­å—äº†å…©é€±çš„åœæ©Ÿæ™‚é–“ã€‚**

This represents a serious process failure.  
é€™ä»£è¡¨åš´é‡çš„æµç¨‹å¤±èª¤ã€‚

#### No Accountability (ç„¡å•è²¬åˆ¶)

There was no formal handover document. No sign-off process. No verification checklist.  
æ²’æœ‰æ­£å¼çš„äº¤æ¥æ–‡ä»¶ã€‚æ²’æœ‰ç°½æ ¸æµç¨‹ã€‚æ²’æœ‰é©—è­‰æª¢æŸ¥æ¸…å–®ã€‚

The vendor said "it's deployed" and we accepted it **without verification**.  
å» å•†èªªã€Œå·²éƒ¨ç½²ã€ï¼Œæˆ‘å€‘**æœªç¶“é©—è­‰å°±æ¥å—äº†**ã€‚

---

### Business Impact (æ¥­å‹™å½±éŸ¿)

| Impact Category | Details |
|-----------------|---------|
| **Service Downtime** | **TWO WEEKS** of complete service unavailability |
| **Single Point of Failure** | **Multiple engineers could not resolve the issue**; only one specific engineer possessed the knowledge to recover the system; critical knowledge centralization risk |
| **Client Business Impact** | **14 days** of Flemabus service operations completely halted; client unable to serve their end customers; massive revenue loss for client |
| **Financial Loss** | Major SLA breach penalties; contract termination risk; potential legal action |
| **Reputation Damage** | Significant damage to client trust; Jasslin's technical competence questioned at executive level; client considering competitor migration |
| **Emergency Response Cost** | Multiple senior engineers diverted for **2 weeks**; all other projects delayed; specific engineer had to personally intervene |
| **Opportunity Cost** | Loss of contract renewal; damaged industry reputation; future client acquisition significantly impacted |
| **Long-term Consequences** | Client relationship permanently damaged; used as cautionary tale in industry; internal credibility crisis |

| å½±éŸ¿é¡åˆ¥ | è©³æƒ… |
|---------|------|
| **æœå‹™åœæ©Ÿæ™‚é–“** | **å…©é€±**çš„å®Œå…¨æœå‹™ä¸å¯ç”¨ |
| **å–®é»æ•…éšœ** | **å¤šåå·¥ç¨‹å¸«ç„¡æ³•è§£æ±ºå•é¡Œ**ï¼›åªæœ‰ç‰¹å®šå·¥ç¨‹å¸«æ“æœ‰æ¢å¾©ç³»çµ±çš„çŸ¥è­˜ï¼›é—œéµçŸ¥è­˜é›†ä¸­åŒ–é¢¨éšª |
| **å®¢æˆ¶æ¥­å‹™å½±éŸ¿** | **14 å¤©**çš„ Flemabus æœå‹™æ¥­å‹™å®Œå…¨ä¸­æ­¢ï¼›å®¢æˆ¶ç„¡æ³•æœå‹™å…¶çµ‚ç«¯å®¢æˆ¶ï¼›å®¢æˆ¶é­å—é‡å¤§æ”¶å…¥æå¤± |
| **è²¡å‹™æå¤±** | é‡å¤§ SLA é•ç´„ç½°æ¬¾ï¼›åˆç´„çµ‚æ­¢é¢¨éšªï¼›æ½›åœ¨æ³•å¾‹è¨´è¨Ÿ |
| **è²è­½æå®³** | å®¢æˆ¶ä¿¡ä»»å—åˆ°é‡å¤§æå®³ï¼›Jasslin æŠ€è¡“èƒ½åŠ›åœ¨é«˜å±¤å—åˆ°è³ªç–‘ï¼›å®¢æˆ¶è€ƒæ…®é·ç§»è‡³ç«¶çˆ­å°æ‰‹ |
| **ç·Šæ€¥æ‡‰å°æˆæœ¬** | å¤šåè³‡æ·±å·¥ç¨‹å¸«è½‰ç§» **2 é€±**ï¼›æ‰€æœ‰å…¶ä»–å°ˆæ¡ˆå»¶é²ï¼›ç‰¹å®šå·¥ç¨‹å¸«å¿…é ˆè¦ªè‡ªä»‹å…¥ |
| **æ©Ÿæœƒæˆæœ¬** | å¤±å»åˆç´„çºŒç´„ï¼›ç”¢æ¥­è²è­½å—æï¼›æœªä¾†å®¢æˆ¶ç²å–åš´é‡å—å½±éŸ¿ |
| **é•·æœŸå¾Œæœ** | å®¢æˆ¶é—œä¿‚æ°¸ä¹…å—æï¼›æˆç‚ºç”¢æ¥­è­¦ç¤ºæ¡ˆä¾‹ï¼›å…§éƒ¨ä¿¡è­½å±æ©Ÿ |

---

### The Lesson (æ•™è¨“)

**This incident was 100% preventable.**  
**æ­¤äº‹æ•…æ˜¯ 100% å¯é é˜²çš„ã€‚**

**The actual failures that caused 2-week downtime:**  
**å°è‡´å…©é€±åœæ©Ÿçš„å¯¦éš›å¤±èª¤ï¼š**

1. **Network naming conflicts** â€” Generic names (app-network) caused new deployment to conflict with production, breaking both systems  
   **ç¶²è·¯å‘½åè¡çª** â€” é€šç”¨åç¨±ï¼ˆapp-networkï¼‰å°è‡´æ–°éƒ¨ç½²èˆ‡ç”Ÿç”¢è¡çªï¼Œå…©å€‹ç³»çµ±éƒ½æå£

2. **Accidental `docker-compose down`** â€” Engineer ran command in wrong directory, brought down production  
   **æ„å¤–åŸ·è¡Œ `docker-compose down`** â€” å·¥ç¨‹å¸«åœ¨éŒ¯èª¤çš„ç›®éŒ„ä¸­é‹è¡Œå‘½ä»¤ï¼Œå°è‡´ç”Ÿç”¢åœæ­¢

3. **No rollback capability** â€” No git tags, no way to restore previous working configuration  
   **ç„¡å›æ»¾èƒ½åŠ›** â€” ç„¡ git æ¨™è¨˜ï¼Œç„¡æ³•æ¢å¾©ä¹‹å‰çš„å·¥ä½œé…ç½®

4. **Knowledge in one person's memory** â€” Only one engineer could diagnose and fix the issues  
   **çŸ¥è­˜åªåœ¨ä¸€å€‹äººè¨˜æ†¶ä¸­** â€” åªæœ‰ä¸€ä½å·¥ç¨‹å¸«èƒ½è¨ºæ–·å’Œä¿®å¾©å•é¡Œ

5. **No documentation** â€” Recovery required reverse-engineering everything from scratch  
   **ç„¡æ–‡ä»¶è¨˜éŒ„** â€” æ¢å¾©éœ€è¦å¾é ­é€†å‘å·¥ç¨‹æ‰€æœ‰å…§å®¹

**Note**: Yes, Docker not enabled and missing restart: always were also issues, but the above failures are what actually extended recovery to TWO WEEKS.  
**æ³¨æ„**ï¼šæ˜¯çš„ï¼ŒDocker æœªå•Ÿç”¨å’Œç¼ºå°‘ restart: always ä¹Ÿæ˜¯å•é¡Œï¼Œä½†ä¸Šè¿°å¤±èª¤æ‰æ˜¯çœŸæ­£å°‡æ¢å¾©å»¶é•·åˆ°å…©é€±çš„åŸå› ã€‚  

**Professional engineering is not optional.**  
**å°ˆæ¥­å·¥ç¨‹ä¸æ˜¯å¯é¸çš„ã€‚**

**Documentation is not bureaucracy; it is the foundation of reliability.**  
**æ–‡ä»¶è¨˜éŒ„ä¸æ˜¯å®˜åƒšä¸»ç¾©ï¼›å®ƒæ˜¯å¯é æ€§çš„åŸºç¤ã€‚**

---

### The Response: A Mandatory Documentation Framework (æ‡‰å°æªæ–½ï¼šå¼·åˆ¶æ€§æ–‡ä»¶æ¡†æ¶)

In response to this incident, this **Documentation Standards Framework** was established as a **non-negotiable requirement** to ensure:  
ç‚ºæ‡‰å°æ­¤äº‹æ•…ï¼Œæ­¤**æ–‡ä»¶æ¨™æº–æ¡†æ¶**è¢«å»ºç«‹ç‚º**ä¸å¯å”å•†çš„è¦æ±‚**ï¼Œä»¥ç¢ºä¿ï¼š

1. **Every system is self-healing** â€” Services survive failures without human intervention  
   **æ¯å€‹ç³»çµ±éƒ½æ˜¯è‡ªæˆ‘æ¢å¾©çš„** â€” æœå‹™åœ¨ç„¡äººç‚ºä»‹å…¥çš„æƒ…æ³ä¸‹ç¶“å¾—èµ·æ•…éšœ

2. **Every deployment is documented** â€” Knowledge exists independently of individual memory; **any engineer can recover the system, not just one person**  
   **æ¯å€‹éƒ¨ç½²éƒ½æœ‰æ–‡ä»¶è¨˜éŒ„** â€” çŸ¥è­˜ç¨ç«‹æ–¼å€‹äººè¨˜æ†¶è€Œå­˜åœ¨ï¼›**ä»»ä½•å·¥ç¨‹å¸«éƒ½èƒ½æ¢å¾©ç³»çµ±ï¼Œè€Œéåƒ…æœ‰ä¸€äºº**

3. **Every change is verified** â€” No system enters production without resilience testing  
   **æ¯å€‹è®Šæ›´éƒ½ç¶“éé©—è­‰** â€” æ²’æœ‰ç³»çµ±åœ¨æœªç¶“éŸŒæ€§æ¸¬è©¦çš„æƒ…æ³ä¸‹é€²å…¥ç”Ÿç”¢ç’°å¢ƒ

4. **Every party is accountable** â€” Clear standards, clear consequences  
   **æ¯ä¸€æ–¹éƒ½è² è²¬** â€” æ˜ç¢ºçš„æ¨™æº–ï¼Œæ˜ç¢ºçš„å¾Œæœ

**This is not a suggestion. It is a requirement.**  
**é€™ä¸æ˜¯å»ºè­°ã€‚é€™æ˜¯è¦æ±‚ã€‚**

**This will never happen again.**  
**é€™å°‡æ°¸ä¸å†ç™¼ç”Ÿã€‚**

---

## Framework Purpose (æ¡†æ¶ç›®çš„)

### Mission Statement (ä½¿å‘½å®£è¨€)

**To ensure all Jasslin managed services are resilient, autonomous, and fully documented, where failures are anticipated, handled automatically, and never result in client-facing incidents.**  
**ç¢ºä¿æ‰€æœ‰ Jasslin è¨—ç®¡æœå‹™å…·æœ‰éŸŒæ€§ã€è‡ªä¸»ä¸”å®Œå…¨è¨˜éŒ„ï¼Œåœ¨æ­¤ç’°å¢ƒä¸­æ•…éšœæ˜¯é æœŸçš„ã€è‡ªå‹•è™•ç†çš„ï¼Œä¸¦ä¸”æ°¸ä¸å°è‡´é¢å‘å®¢æˆ¶çš„äº‹æ•…ã€‚**

### Core Principles (æ ¸å¿ƒåŸå‰‡)

#### 1. Resilience by Design (è¨­è¨ˆéŸŒæ€§)

Systems must be **architected to survive failures**, not merely "hoped" to work.  
ç³»çµ±å¿…é ˆ**è¨­è¨ˆç‚ºèƒ½ç¶“å¾—èµ·æ•…éšœ**ï¼Œè€Œä¸åƒ…åƒ…æ˜¯ã€Œå¸Œæœ›ã€èƒ½é‹ä½œã€‚

- All services self-heal automatically  
  æ‰€æœ‰æœå‹™è‡ªå‹•è‡ªæˆ‘æ¢å¾©
- Failures are isolated and contained  
  æ•…éšœè¢«éš”é›¢å’Œæ§åˆ¶
- Recovery is measurable and verifiable  
  æ¢å¾©æ˜¯å¯è¡¡é‡å’Œå¯é©—è­‰çš„

#### 2. Documentation as Code (æ–‡ä»¶å³ç¨‹å¼ç¢¼)

**"If it is not documented, it does not exist."**  
**ã€Œå¦‚æœæ²’æœ‰æ–‡ä»¶è¨˜éŒ„ï¼Œå®ƒå°±ä¸å­˜åœ¨ã€‚ã€**

Documentation is not an afterthought; it is a **first-class deliverable** with the same importance as code.  
æ–‡ä»¶è¨˜éŒ„ä¸æ˜¯äº‹å¾Œæƒ³æ³•ï¼›å®ƒæ˜¯èˆ‡ç¨‹å¼ç¢¼åŒç­‰é‡è¦çš„**ä¸€æµäº¤ä»˜ç‰©**ã€‚

**Critical system knowledge must NEVER exist in only one person's memory.** The incident proved that knowledge centralization is a critical single point of failure.  
**é—œéµç³»çµ±çŸ¥è­˜çµ•ä¸èƒ½åªå­˜åœ¨æ–¼ä¸€å€‹äººçš„è¨˜æ†¶ä¸­ã€‚**äº‹æ•…è­‰æ˜äº†çŸ¥è­˜é›†ä¸­åŒ–æ˜¯é—œéµå–®é»æ•…éšœã€‚

#### 3. Zero Trust Operations (é›¶ä¿¡ä»»é‹ç‡Ÿ)

We do not trust:  
æˆ‘å€‘ä¸ä¿¡ä»»ï¼š
- âŒ Human memory  
- âŒ Verbal handovers  
- âŒ "Experienced" intuition  

We trust:  
æˆ‘å€‘ä¿¡ä»»ï¼š
- âœ… Written, tested procedures  
- âœ… Automated verification  
- âœ… Measurable outcomes  

#### 4. Universal Accountability (æ™®éå•è²¬)

These standards apply **equally** to:  
é€™äº›æ¨™æº–**åŒç­‰**é©ç”¨æ–¼ï¼š
- Jasslin internal teams  
  Jasslin å…§éƒ¨åœ˜éšŠ
- Third-party vendors  
  ç¬¬ä¸‰æ–¹å» å•†
- Senior engineers and junior developers  
  è³‡æ·±å·¥ç¨‹å¸«å’Œåˆç´šé–‹ç™¼è€…

**No one is exempt. No exceptions are granted.**  
**ç„¡äººè±å…ã€‚ä¸æˆäºˆä¾‹å¤–ã€‚**

---

## Core Standards (æ ¸å¿ƒæ¨™æº–)

This framework defines two levels of requirements:

1. **Hard Gates (ç¡¬æ€§é–˜é–€)** - Automated checks that block merge/release
2. **Aspirational Standards (å»ºè­°æ¨™æº–)** - Best practices that improve quality but don't block deployment

æœ¬æ¡†æ¶å®šç¾©å…©ç´šè¦æ±‚ï¼š

1. **Hard Gates (ç¡¬æ€§é–˜é–€)** - é˜»æ­¢ merge/release çš„è‡ªå‹•åŒ–æª¢æŸ¥
2. **Aspirational Standards (å»ºè­°æ¨™æº–)** - æå‡å“è³ªä½†ä¸é˜»æ­¢éƒ¨ç½²çš„æœ€ä½³å¯¦è¸

---

### Standard #1: Environment Isolation and Naming (ç’°å¢ƒéš”é›¢èˆ‡å‘½åè¦ç¯„)

#### ğŸ”´ Hard Gate: Unique Resource Naming

**Requirement**: All Docker resources MUST use project-specific names to prevent conflicts.  
**è¦æ±‚**ï¼šæ‰€æœ‰ Docker è³‡æºå¿…é ˆä½¿ç”¨å°ˆæ¡ˆç‰¹å®šåç¨±ä»¥é˜²æ­¢è¡çªã€‚

**The Problem This Solves**: The incident was caused by network name conflicts when new deployment tried to use same network name as existing production, breaking BOTH systems.  
**é€™è§£æ±ºçš„å•é¡Œ**ï¼šäº‹æ•…ç”±æ–°éƒ¨ç½²å˜—è©¦ä½¿ç”¨èˆ‡ç¾æœ‰ç”Ÿç”¢ç›¸åŒçš„ç¶²è·¯åç¨±å°è‡´çš„ç¶²è·¯è¡çªå¼•èµ·ï¼Œä½¿å…©å€‹ç³»çµ±éƒ½æå£ã€‚

**Automated Checks (CI/CD Pipeline):**

```bash
# Check 1: Networks must have project-specific names (not generic "app-network")
! grep -E "networks:.*\n.*[^a-z0-9_-]*(app-network|default|web|backend)\s*:" docker-compose.yml

# Check 2: Container names must include project prefix
grep -q "container_name:.*\${PROJECT_NAME}" docker-compose.yml || exit 1

# Check 3: Network names must be defined with project prefix
grep -q "networks:.*\n.*${PROJECT_NAME}" docker-compose.yml || exit 1
```

**What blocks merge/release:**
- âŒ Generic network names (app-network, default, web, backend)
- âŒ Missing project prefix in container names
- âŒ No custom network definition

**è‡ªå‹•åŒ–æª¢æŸ¥ï¼ˆCI/CD æµç¨‹ï¼‰ï¼š**

**é˜»æ­¢ merge/release çš„æ¢ä»¶ï¼š**
- âŒ é€šç”¨ç¶²è·¯åç¨±ï¼ˆapp-network, default, web, backendï¼‰
- âŒ å®¹å™¨åç¨±ä¸­ç¼ºå°‘å°ˆæ¡ˆå‰ç¶´
- âŒ ç„¡è‡ªè¨‚ç¶²è·¯å®šç¾©

**Correct Example:**

```yaml
services:
  api:
    container_name: flemabus-api  # ğŸ”´ Hard Gate: Must include project name
    networks:
      - flemabus-network          # ğŸ”´ Hard Gate: Project-specific name
    restart: always

networks:
  flemabus-network:               # ğŸ”´ Hard Gate: Explicitly defined
    driver: bridge
```

**Why This Matters**: Without this, running `docker-compose up` in different projects can:
- Conflict with existing networks
- Break running containers
- Make recovery impossible without documentation

**ç‚ºä½•é‡è¦**ï¼šæ²’æœ‰é€™å€‹ï¼Œåœ¨ä¸åŒå°ˆæ¡ˆä¸­é‹è¡Œ `docker-compose up` å¯èƒ½ï¼š
- èˆ‡ç¾æœ‰ç¶²è·¯è¡çª
- ç ´å£é‹è¡Œä¸­çš„å®¹å™¨
- ä½¿æ¢å¾©è®Šå¾—ä¸å¯èƒ½ï¼ˆç„¡æ–‡ä»¶è¨˜éŒ„ï¼‰

---

### Standard #2: No Manual Destructive Operations (ç¦æ­¢æ‰‹å‹•ç ´å£æ€§æ“ä½œ)

#### ğŸ”´ Hard Gate: All Deployments via Git

**Requirement**: Production changes must go through git-tracked docker-compose files. No manual `docker-compose down`.  
**è¦æ±‚**ï¼šç”Ÿç”¢è®Šæ›´å¿…é ˆé€šé git è¿½è¹¤çš„ docker-compose æª”æ¡ˆã€‚ç¦æ­¢æ‰‹å‹• `docker-compose down`ã€‚

**The Problem This Solves**: Engineer manually ran `docker-compose down` in wrong directory, bringing down production for TWO WEEKS.  
**é€™è§£æ±ºçš„å•é¡Œ**ï¼šå·¥ç¨‹å¸«åœ¨éŒ¯èª¤çš„ç›®éŒ„ä¸­æ‰‹å‹•é‹è¡Œ `docker-compose down`ï¼Œå°è‡´ç”Ÿç”¢åœæ­¢å…©é€±ã€‚

**Automated Checks:**

```bash
# Check: docker-compose.yml must be in git
git ls-files docker-compose.yml | grep -q docker-compose.yml || exit 1

# Check: Must have PROJECT_NAME in .env for identification
grep -q "^PROJECT_NAME=" .env || exit 1
```

**What blocks merge/release:**
- âŒ docker-compose.yml not tracked in git
- âŒ Missing PROJECT_NAME in .env

**é˜»æ­¢ merge/release çš„æ¢ä»¶ï¼š**
- âŒ docker-compose.yml æœªåœ¨ git ä¸­è¿½è¹¤
- âŒ .env ä¸­ç¼ºå°‘ PROJECT_NAME

**Correct Workflow:**

```bash
# âœ… CORRECT: Deploy via git
cd /opt/flemabus
git pull
docker-compose up -d

# âŒ WRONG: Manual operations without git
cd /some/directory
docker-compose down  # Could be wrong directory!
```

**Protection Mechanism**: Add comment to docker-compose.yml

```yaml
# PROJECT: Flemabus Production
# âš ï¸ DO NOT manually docker-compose down
# âš ï¸ All changes must go through git pull
services:
  # ...
```

---

### Standard #3: Configuration Backup and Rollback (é…ç½®å‚™ä»½èˆ‡å›æ»¾)

#### ğŸ”´ Hard Gate: Git Tags for Deployments

**Requirement**: All production deployments must be tagged in git for rollback capability.  
**è¦æ±‚**ï¼šæ‰€æœ‰ç”Ÿç”¢éƒ¨ç½²å¿…é ˆåœ¨ git ä¸­æ¨™è¨˜ä»¥å…·å‚™å›æ»¾èƒ½åŠ›ã€‚

**The Problem This Solves**: When system went down, recovery took TWO WEEKS because no one knew the previous working configuration.  
**é€™è§£æ±ºçš„å•é¡Œ**ï¼šç•¶ç³»çµ±åœæ­¢æ™‚ï¼Œæ¢å¾©èŠ±äº†å…©é€±æ™‚é–“ï¼Œå› ç‚ºæ²’äººçŸ¥é“ä¹‹å‰çš„å·¥ä½œé…ç½®ã€‚

**Automated Checks:**

```bash
# Check: Most recent commit must be tagged
git describe --exact-match HEAD 2>/dev/null || {
  echo "âŒ HEAD commit must be tagged before production deployment"
  exit 1
}

# Check: Tag must follow version format
git describe --exact-match HEAD | grep -E "^v[0-9]+\.[0-9]+\.[0-9]+$" || exit 1
```

**What blocks merge/release:**
- âŒ Deploying untagged commits to production
- âŒ Tags not following version format (v1.0.0)

**é˜»æ­¢ merge/release çš„æ¢ä»¶ï¼š**
- âŒ éƒ¨ç½²æœªæ¨™è¨˜çš„æäº¤åˆ°ç”Ÿç”¢ç’°å¢ƒ
- âŒ æ¨™è¨˜æœªéµå¾ªç‰ˆæœ¬æ ¼å¼ï¼ˆv1.0.0ï¼‰

**Correct Workflow:**

```bash
# Before production deployment:
git tag -a v1.2.3 -m "Production release 2026-02-02"
git push origin v1.2.3

# Deploy
cd /opt/flemabus
git fetch --tags
git checkout v1.2.3
docker-compose up -d

# Rollback (when needed):
git checkout v1.2.2  # Previous known good version
docker-compose up -d
```

**Why This Matters**: With git tags, rollback takes 30 seconds. Without tags, recovery took TWO WEEKS.  
**ç‚ºä½•é‡è¦**ï¼šæœ‰ git æ¨™è¨˜ï¼Œå›æ»¾éœ€è¦ 30 ç§’ã€‚æ²’æœ‰æ¨™è¨˜ï¼Œæ¢å¾©èŠ±äº†å…©é€±ã€‚

---

### Standard #4: Service Persistence (æœå‹™æŒä¹…æ€§æ¨™æº–)

#### ğŸ”´ Hard Gate: Automated Configuration Check

**Requirement**: All production services must be configured to survive reboots.  
**è¦æ±‚**ï¼šæ‰€æœ‰ç”Ÿç”¢æœå‹™å¿…é ˆé…ç½®ç‚ºèƒ½ç¶“å¾—èµ·é‡å•Ÿã€‚

**Automated Checks (CI/CD Pipeline):**

```bash
# Check 1: Docker compose file has restart policies
grep -q "restart: always" docker-compose.yml || exit 1

# Check 2: Health checks are defined
grep -q "healthcheck:" docker-compose.yml || exit 1
```

**What blocks merge/release:**
- âŒ Missing `restart: always` in docker-compose.yml
- âŒ No health check configuration

**è‡ªå‹•åŒ–æª¢æŸ¥ï¼ˆCI/CD æµç¨‹ï¼‰ï¼š**

```bash
# æª¢æŸ¥ 1: Docker compose æª”æ¡ˆæœ‰é‡å•Ÿç­–ç•¥
grep -q "restart: always" docker-compose.yml || exit 1

# æª¢æŸ¥ 2: å·²å®šç¾©å¥åº·æª¢æŸ¥
grep -q "healthcheck:" docker-compose.yml || exit 1
```

**é˜»æ­¢ merge/release çš„æ¢ä»¶ï¼š**
- âŒ docker-compose.yml ä¸­ç¼ºå°‘ `restart: always`
- âŒ ç„¡å¥åº·æª¢æŸ¥é…ç½®

**Example Configuration:**

```yaml
services:
  api-service:
    image: your-service:latest
    restart: always  # ğŸ”´ Hard Gate: Must be present
    healthcheck:      # ğŸ”´ Hard Gate: Must be present
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

#### ğŸŸ¡ Aspirational: Hard Reboot Testing

**Recommended Practice**: Test actual reboot recovery in staging.  
**å»ºè­°å¯¦è¸**ï¼šåœ¨é ç™¼ç’°å¢ƒæ¸¬è©¦å¯¦éš›é‡å•Ÿæ¢å¾©ã€‚

```bash
# On staging server:
sudo reboot now
# Verify all containers restart automatically
```

**Note**: This is a best practice but not a deployment blocker if you don't have staging environment access.  
**æ³¨æ„**ï¼šé€™æ˜¯æœ€ä½³å¯¦è¸ï¼Œä½†å¦‚æœæ‚¨æ²’æœ‰é ç™¼ç’°å¢ƒå­˜å–æ¬Šé™ï¼Œä¸æœƒé˜»æ­¢éƒ¨ç½²ã€‚  

---

### Standard #5: Documentation Requirement (æ–‡ä»¶è¨˜éŒ„è¦æ±‚)

#### ğŸ”´ Hard Gate: Documentation File Existence

**Requirement**: Core documentation files must exist in `/docs` folder.  
**è¦æ±‚**ï¼šæ ¸å¿ƒæ–‡ä»¶æª”æ¡ˆå¿…é ˆå­˜åœ¨æ–¼ `/docs` è³‡æ–™å¤¾ä¸­ã€‚

**Automated Checks (CI/CD Pipeline):**

```bash
# Check: All required documentation files exist
test -f docs/ARCHITECTURE.md || exit 1
test -f docs/DEPLOY.md || exit 1
test -f docs/RESILIENCE.md || exit 1
test -f docs/TEST_REPORT.md || exit 1
```

**What blocks merge/release:**
- âŒ Missing any of the 4 core documentation files in `/docs/`

**è‡ªå‹•åŒ–æª¢æŸ¥ï¼ˆCI/CD æµç¨‹ï¼‰ï¼š**

```bash
# æª¢æŸ¥ï¼šæ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶æª”æ¡ˆå­˜åœ¨
test -f docs/ARCHITECTURE.md || exit 1
test -f docs/DEPLOY.md || exit 1
test -f docs/RESILIENCE.md || exit 1
test -f docs/TEST_REPORT.md || exit 1
```

**é˜»æ­¢ merge/release çš„æ¢ä»¶ï¼š**
- âŒ `/docs/` ä¸­ç¼ºå°‘ 4 å€‹æ ¸å¿ƒæ–‡ä»¶æª”æ¡ˆä¸­çš„ä»»ä½•ä¸€å€‹

**Required Files:**
- `docs/ARCHITECTURE.md` - System blueprint
- `docs/DEPLOY.md` - Deployment steps
- `docs/RESILIENCE.md` - Recovery procedures  
- `docs/TEST_REPORT.md` - Test results template

#### ğŸŸ¡ Aspirational: Documentation Quality Standards

**Recommended Practices** (but not deployment blockers):  
**å»ºè­°å¯¦è¸**ï¼ˆä½†ä¸é˜»æ­¢éƒ¨ç½²ï¼‰ï¼š

- ğŸ“ Bilingual documentation (English + Chinese)  
  ğŸ“ é›™èªæ–‡ä»¶ï¼ˆè‹±æ–‡ + ä¸­æ–‡ï¼‰

- ğŸ“ Complete environment variable tables  
  ğŸ“ å®Œæ•´çš„ç’°å¢ƒè®Šæ•¸è¡¨

- ğŸ“ Mermaid diagrams for architecture  
  ğŸ“ æ¶æ§‹çš„ Mermaid åœ–è¡¨

- ğŸ“ Detailed rollback procedures  
  ğŸ“ è©³ç´°çš„å›æ»¾ç¨‹åº

**Note**: The incident showed that knowledge concentration is a critical risk. While we cannot enforce who can understand your documentation, writing clear deployment steps helps avoid single points of failure.  
**æ³¨æ„**ï¼šäº‹æ•…é¡¯ç¤ºçŸ¥è­˜é›†ä¸­åŒ–æ˜¯é—œéµé¢¨éšªã€‚é›–ç„¶æˆ‘å€‘ç„¡æ³•å¼·åˆ¶è¦æ±‚èª°èƒ½ç†è§£æ‚¨çš„æ–‡ä»¶ï¼Œä½†ç·¨å¯«æ¸…æ™°çš„éƒ¨ç½²æ­¥é©Ÿæœ‰åŠ©æ–¼é¿å…å–®é»æ•…éšœã€‚

---

## Definition of Done (DoD) (é©—æ”¶æ¨™æº–)

### ğŸ”´ Hard Gates (Blocks Merge/Release)

**These checks MUST pass before code can be merged or released:**  
**é€™äº›æª¢æŸ¥å¿…é ˆåœ¨ç¨‹å¼ç¢¼åˆä½µæˆ–ç™¼å¸ƒå‰é€šéï¼š**

#### Standard #1: Environment Isolation
- [ ] No generic network names (app-network, default, web, backend)
- [ ] Container names include project prefix
- [ ] Custom networks explicitly defined

#### Standard #2: No Manual Destructive Operations
- [ ] docker-compose.yml tracked in git
- [ ] PROJECT_NAME defined in .env

#### Standard #3: Configuration Backup
- [ ] HEAD commit is tagged with version
- [ ] Tag follows format v1.0.0

#### Standard #4: Service Persistence
- [ ] All services have `restart: always`
- [ ] All critical services have healthcheck

#### Standard #5: Documentation
- [ ] All 4 documentation files exist

**Automated check script:**

```bash
#!/bin/bash
# Pre-merge validation script - Lessons from the 2-week outage

echo "Running Hard Gate checks (lessons from the incident)..."

# Standard #1: Environment Isolation (prevents network conflicts)
echo "Checking environment isolation..."
if grep -E "networks:.*\n.*[^a-z0-9_-]*(app-network|default|web|backend)\s*:" docker-compose.yml; then
  echo "âŒ Generic network names found - use project-specific names"
  exit 1
fi

grep -q "container_name:" docker-compose.yml || { echo "âŒ Missing container_name"; exit 1; }
grep -q "networks:" docker-compose.yml || { echo "âŒ Missing custom networks definition"; exit 1; }

# Standard #2: No Manual Operations (prevents accidental docker-compose down)
echo "Checking git tracking..."
git ls-files docker-compose.yml | grep -q docker-compose.yml || { 
  echo "âŒ docker-compose.yml not in git"; exit 1; 
}
grep -q "^PROJECT_NAME=" .env || { echo "âŒ Missing PROJECT_NAME in .env"; exit 1; }

# Standard #3: Rollback Capability (prevents 2-week recovery)
echo "Checking version tagging..."
git describe --exact-match HEAD 2>/dev/null || {
  echo "âŒ HEAD not tagged - tag with: git tag -a v1.0.0 -m 'Release'"
  exit 1
}

# Standard #4: Service Persistence (survives reboot)
echo "Checking service persistence..."
grep -q "restart: always" docker-compose.yml || { echo "âŒ Missing restart: always"; exit 1; }
grep -q "healthcheck:" docker-compose.yml || { echo "âŒ Missing healthcheck"; exit 1; }

# Standard #5: Documentation (eliminates single point of knowledge)
echo "Checking documentation..."
test -f docs/ARCHITECTURE.md || { echo "âŒ Missing ARCHITECTURE.md"; exit 1; }
test -f docs/DEPLOY.md || { echo "âŒ Missing DEPLOY.md"; exit 1; }
test -f docs/RESILIENCE.md || { echo "âŒ Missing RESILIENCE.md"; exit 1; }
test -f docs/TEST_REPORT.md || { echo "âŒ Missing TEST_REPORT.md"; exit 1; }

echo "âœ… All Hard Gates passed"
echo "   These checks prevent: network conflicts, accidental shutdowns, 2-week recovery time"
```

### ğŸŸ¡ Aspirational Standards (Recommended but not blockers)

**These improve quality but won't block deployment:**  
**é€™äº›æå‡å“è³ªä½†ä¸æœƒé˜»æ­¢éƒ¨ç½²ï¼š**

- Unit test coverage >80%
- Bilingual documentation (English + Chinese)
- Hard reboot test performed on staging
- Performance benchmarks documented
- QA sign-off obtained

- [ ] All health checks operational  
      æ‰€æœ‰å¥åº·æª¢æŸ¥é‹ä½œæ­£å¸¸

- [ ] Performance benchmarks meet SLA requirements  
      æ•ˆèƒ½åŸºæº–ç¬¦åˆ SLA è¦æ±‚

- [ ] `TEST_REPORT.md` completed with results and screenshots  
      `TEST_REPORT.md` å·²å®Œæˆï¼ŒåŒ…å«çµæœå’Œæˆªåœ–

- [ ] Sign-off obtained from DevOps Lead and QA Lead  
      å·²ç²å¾— DevOps è² è²¬äººå’Œ QA è² è²¬äººçš„ç°½ç½²

### Quick Reference: Hard Gates Checklist (å¿«é€Ÿåƒè€ƒï¼šç¡¬æ€§é–˜é–€æª¢æŸ¥æ¸…å–®)

```bash
# Run this before submitting PR:
bash scripts/validate-hardgates.sh

# What it checks:
âœ“ Project-specific network names (not "app-network")  
âœ“ Container names have project prefix
âœ“ docker-compose.yml in git
âœ“ PROJECT_NAME in .env  
âœ“ Current commit is tagged (v1.0.0 format)
âœ“ restart: always present
âœ“ healthcheck present
âœ“ 4 documentation files exist
```

### Summary: What Blocks Merge/Release (ç¸½çµï¼šä»€éº¼æœƒé˜»æ­¢åˆä½µ/ç™¼å¸ƒ)

**Pull requests will not be merged if Hard Gates fail:**  
**å¦‚æœç¡¬æ€§é–˜é–€å¤±æ•—ï¼Œæ‹‰å–è«‹æ±‚å°‡ä¸æœƒè¢«åˆä½µï¼š**

**From the 2-week outage, these checks prevent:**

#### Standard #1: Environment Isolation
- âŒ Generic network names (prevents network conflicts that broke both systems)
- âŒ Missing container_name with project prefix
- âŒ No custom network definition

#### Standard #2: No Manual Operations  
- âŒ docker-compose.yml not in git (prevents accidental wrong-directory operations)
- âŒ Missing PROJECT_NAME in .env

#### Standard #3: Rollback Capability
- âŒ Untagged git commits (prevents 2-week recovery time)
- âŒ Tag format not v1.0.0

#### Standard #4: Service Persistence
- âŒ Missing `restart: always` 
- âŒ Missing health check configuration

#### Standard #5: Documentation
- âŒ Missing any of the 4 documentation files (prevents single-point-of-knowledge)

**Everything else is recommended but won't block deployment.**  
**å…¶ä»–æ‰€æœ‰å…§å®¹éƒ½æ˜¯å»ºè­°ä½†ä¸æœƒé˜»æ­¢éƒ¨ç½²ã€‚**

---

## Documentation Structure (æ–‡ä»¶è¨˜éŒ„çµæ§‹)

### Repository Layout (å„²å­˜åº«ä½ˆå±€)

```
documentation-management/
â”œâ”€â”€ README.md                    # This file - Documentation Standards
â”‚                                # æœ¬æª”æ¡ˆ - æ–‡ä»¶æ¨™æº–
â”‚
â”œâ”€â”€ docs/                        # âš ï¸ MANDATORY DIRECTORY - Core Documentation
â”‚   â”‚                            # âš ï¸ å¼·åˆ¶æ€§ç›®éŒ„ - æ ¸å¿ƒæ–‡ä»¶è¨˜éŒ„
â”‚   â”‚
â”‚   â”œâ”€â”€ ARCHITECTURE.md          # System Facts (ç³»çµ±äº‹å¯¦)
â”‚   â”‚   â”œâ”€â”€ Service inventory with ports and dependencies
â”‚   â”‚   â”œâ”€â”€ System diagrams (Mermaid.js)
â”‚   â”‚   â”œâ”€â”€ Third-party API dependencies
â”‚   â”‚   â”œâ”€â”€ Security architecture
â”‚   â”‚   â””â”€â”€ Disaster recovery specifications
â”‚   â”‚
â”‚   â”œâ”€â”€ DEPLOY.md                # Environment SOP (ç’°å¢ƒæ¨™æº–ä½œæ¥­ç¨‹åº)
â”‚   â”‚   â”œâ”€â”€ Prerequisites and system requirements
â”‚   â”‚   â”œâ”€â”€ Environment variable configuration table
â”‚   â”‚   â”œâ”€â”€ Step-by-step deployment sequence
â”‚   â”‚   â”œâ”€â”€ Volume mounting rules
â”‚   â”‚   â””â”€â”€ Deployment verification checklist
â”‚   â”‚
â”‚   â”œâ”€â”€ RESILIENCE.md            # Self-Healing Configuration (è‡ªæˆ‘æ¢å¾©é…ç½®)
â”‚   â”‚   â”œâ”€â”€ Docker auto-start configuration
â”‚   â”‚   â”œâ”€â”€ Standard docker-compose.yml with restart policies
â”‚   â”‚   â”œâ”€â”€ Health check configuration
â”‚   â”‚   â”œâ”€â”€ Recovery SOP for common failures
â”‚   â”‚   â””â”€â”€ Monitoring and alerting setup
â”‚   â”‚
â”‚   â””â”€â”€ TEST_REPORT.md           # Staging Verification (é ç™¼ç’°å¢ƒé©—è­‰)
â”‚       â”œâ”€â”€ Hard Reboot Test checklist and results
â”‚       â”œâ”€â”€ Functional test results
â”‚       â”œâ”€â”€ Performance benchmarks
â”‚       â”œâ”€â”€ Three rollback methods (Git, Docker, Database)
â”‚       â””â”€â”€ Sign-off section with stakeholder approval
â”‚
â”œâ”€â”€ src/                         # Application source code (æ‡‰ç”¨ç¨‹å¼åŸå§‹ç¢¼)
â”œâ”€â”€ config/                      # Configuration files (é…ç½®æª”æ¡ˆ)
â”œâ”€â”€ docker-compose.yml           # Container orchestration (å®¹å™¨ç·¨æ’)
â”œâ”€â”€ .env.example                 # Environment template (ç’°å¢ƒç¯„æœ¬)
â””â”€â”€ [other project files]        # å…¶ä»–å°ˆæ¡ˆæª”æ¡ˆ
```

### The 4 Mandatory Files (4 å€‹å¼·åˆ¶æ€§æª”æ¡ˆ)

#### 1. ARCHITECTURE.md â€” System Facts (ç³»çµ±äº‹å¯¦)

**Purpose**: Provide a complete, accurate technical blueprint of the system.  
**ç›®çš„**ï¼šæä¾›ç³»çµ±çš„å®Œæ•´ã€æº–ç¢ºçš„æŠ€è¡“è—åœ–ã€‚

**Key Sections**:
- System overview with Mermaid diagrams  
  åŒ…å« Mermaid åœ–è¡¨çš„ç³»çµ±æ¦‚è¦½

- Service list: Name, Port, Purpose, Dependencies, Health Check Endpoint  
  æœå‹™æ¸…å–®ï¼šåç¨±ã€ç«¯å£ã€ç›®çš„ã€ä¾è³´é—œä¿‚ã€å¥åº·æª¢æŸ¥ç«¯é»

- Third-party API dependencies with rate limits and fallback strategies  
  ç¬¬ä¸‰æ–¹ API ä¾è³´é—œä¿‚ï¼ŒåŒ…å«é€Ÿç‡é™åˆ¶å’Œå‚™æ´ç­–ç•¥

- Network architecture and security zones  
  ç¶²è·¯æ¶æ§‹å’Œå®‰å…¨å€åŸŸ

- Disaster recovery: RTO/RPO, backup frequency, retention  
  ç½é›£æ¢å¾©ï¼šRTO/RPOã€å‚™ä»½é »ç‡ã€ä¿ç•™æœŸ

**This document answers**: "What is this system?"  
**æ­¤æ–‡ä»¶å›ç­”**ï¼šã€Œé€™å€‹ç³»çµ±æ˜¯ä»€éº¼ï¼Ÿã€

#### 2. DEPLOY.md â€” Environment SOP (ç’°å¢ƒæ¨™æº–ä½œæ¥­ç¨‹åº)

**Purpose**: Enable any engineer to deploy the system from scratch using only this document.  
**ç›®çš„**ï¼šä½¿ä»»ä½•å·¥ç¨‹å¸«åƒ…ä½¿ç”¨æ­¤æ–‡ä»¶å³å¯å¾é›¶é–‹å§‹éƒ¨ç½²ç³»çµ±ã€‚

**Key Sections**:
- System requirements and prerequisites  
  ç³»çµ±è¦æ±‚å’Œå‰ç½®æ¢ä»¶

- Environment variable table: Key, Description, Example, Required (Y/N)  
  ç’°å¢ƒè®Šæ•¸è¡¨ï¼šéµã€æè¿°ã€ç¯„ä¾‹ã€å¿…éœ€ï¼ˆæ˜¯/å¦ï¼‰

- 12-step deployment sequence from OS setup to health check verification  
  å¾ä½œæ¥­ç³»çµ±è¨­å®šåˆ°å¥åº·æª¢æŸ¥é©—è­‰çš„ 12 æ­¥éƒ¨ç½²åºåˆ—

- Volume mounting rules with host/container path mappings  
  åŒ…å«ä¸»æ©Ÿ/å®¹å™¨è·¯å¾‘æ˜ å°„çš„å·æ›è¼‰è¦å‰‡

- Deployment verification checklist  
  éƒ¨ç½²é©—è­‰æª¢æŸ¥æ¸…å–®

**This document answers**: "How do I deploy this system?"  
**æ­¤æ–‡ä»¶å›ç­”**ï¼šã€Œæˆ‘å¦‚ä½•éƒ¨ç½²é€™å€‹ç³»çµ±ï¼Ÿã€

#### 3. RESILIENCE.md â€” Self-Healing Configuration (è‡ªæˆ‘æ¢å¾©é…ç½®)

**Purpose**: Define how the system survives failures and recovers automatically.  
**ç›®çš„**ï¼šå®šç¾©ç³»çµ±å¦‚ä½•ç¶“å¾—èµ·æ•…éšœä¸¦è‡ªå‹•æ¢å¾©ã€‚

**Key Sections**:
- Docker service enablement verification commands  
  Docker æœå‹™å•Ÿç”¨é©—è­‰å‘½ä»¤

- Complete `docker-compose.yml` example with `restart: always` on all services  
  å®Œæ•´çš„ `docker-compose.yml` ç¯„ä¾‹ï¼Œæ‰€æœ‰æœå‹™éƒ½æœ‰ `restart: always`

- Health check configuration best practices  
  å¥åº·æª¢æŸ¥é…ç½®æœ€ä½³å¯¦è¸

- Recovery SOP table: Failure Scenario â†’ Detection â†’ Recovery Steps â†’ Prevention  
  æ¢å¾© SOP è¡¨ï¼šæ•…éšœæƒ…å¢ƒ â†’ åµæ¸¬ â†’ æ¢å¾©æ­¥é©Ÿ â†’ é é˜²

- Automated recovery scripts and monitoring integration  
  è‡ªå‹•æ¢å¾©è…³æœ¬å’Œç›£æ§æ•´åˆ

**This document answers**: "How does this system survive failures?"  
**æ­¤æ–‡ä»¶å›ç­”**ï¼šã€Œé€™å€‹ç³»çµ±å¦‚ä½•ç¶“å¾—èµ·æ•…éšœï¼Ÿã€

#### 4. TEST_REPORT.md â€” Staging Verification (é ç™¼ç’°å¢ƒé©—è­‰)

**Purpose**: Prove that the system meets IronGate standards before production deployment.  
**ç›®çš„**ï¼šè­‰æ˜ç³»çµ±åœ¨ç”Ÿç”¢éƒ¨ç½²å‰ç¬¦åˆéµé–˜æ¨™æº–ã€‚

**Key Sections**:
- Environment information (staging server details, versions, commit hash)  
  ç’°å¢ƒè³‡è¨Šï¼ˆé ç™¼ç’°å¢ƒä¼ºæœå™¨è©³æƒ…ã€ç‰ˆæœ¬ã€æäº¤é›œæ¹Šï¼‰

- **Hard Reboot Test checklist** (11 items) with Pass/Fail status  
  **ç¡¬é‡å•Ÿæ¸¬è©¦æª¢æŸ¥æ¸…å–®**ï¼ˆ11 é …ï¼‰åŒ…å«é€šé/å¤±æ•—ç‹€æ…‹

- Functional test results (API endpoints, database, security)  
  åŠŸèƒ½æ¸¬è©¦çµæœï¼ˆAPI ç«¯é»ã€è³‡æ–™åº«ã€å®‰å…¨æ€§ï¼‰

- Performance test results (load, stress, spike tests)  
  æ•ˆèƒ½æ¸¬è©¦çµæœï¼ˆè² è¼‰ã€å£“åŠ›ã€å°–å³°æ¸¬è©¦ï¼‰

- Three rollback methods with actual shell commands  
  ä¸‰ç¨®å›æ»¾æ–¹æ³•ï¼ŒåŒ…å«å¯¦éš›çš„ shell å‘½ä»¤

- Sign-off table: DevOps Lead, QA Lead, Backend Lead, Product Manager  
  ç°½ç½²è¡¨ï¼šDevOps è² è²¬äººã€QA è² è²¬äººã€å¾Œç«¯è² è²¬äººã€ç”¢å“ç¶“ç†

**This document answers**: "Is this system ready for production?"  
**æ­¤æ–‡ä»¶å›ç­”**ï¼šã€Œé€™å€‹ç³»çµ±æ˜¯å¦æº–å‚™å¥½ç”Ÿç”¢ï¼Ÿã€

---

## Enforcement and Compliance (åŸ·è¡Œèˆ‡åˆè¦)

### Pre-Deployment Gate (éƒ¨ç½²å‰é–˜é–€)

Before ANY production deployment, the following checklist MUST be completed and verified:  
åœ¨ä»»ä½•ç”Ÿç”¢éƒ¨ç½²ä¹‹å‰ï¼Œå¿…é ˆå®Œæˆä¸¦é©—è­‰ä»¥ä¸‹æª¢æŸ¥æ¸…å–®ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PRE-DEPLOYMENT CHECKLIST                                â”‚
â”‚  éƒ¨ç½²å‰æª¢æŸ¥æ¸…å–®                                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  DOCUMENTATION (æ–‡ä»¶è¨˜éŒ„)                                 â”‚
â”‚  â˜ ARCHITECTURE.md exists and is complete               â”‚
â”‚  â˜ DEPLOY.md exists and is complete                     â”‚
â”‚  â˜ RESILIENCE.md exists and is complete                 â”‚
â”‚  â˜ TEST_REPORT.md exists and is signed off              â”‚
â”‚                                                          â”‚
â”‚  SERVICE PERSISTENCE (æœå‹™æŒä¹…æ€§)                         â”‚
â”‚  â˜ Docker enabled: systemctl is-enabled docker = enabledâ”‚
â”‚  â˜ All services have restart: always                    â”‚
â”‚  â˜ Health checks configured for all critical services   â”‚
â”‚                                                          â”‚
â”‚  STAGING VERIFICATION (é ç™¼ç’°å¢ƒé©—è­‰)                      â”‚
â”‚  â˜ Hard Reboot Test performed and PASSED                â”‚
â”‚  â˜ All containers restarted automatically after reboot  â”‚
â”‚  â˜ No manual intervention was required                  â”‚
â”‚  â˜ Performance benchmarks meet SLA requirements         â”‚
â”‚                                                          â”‚
â”‚  ACCOUNTABILITY (å•è²¬åˆ¶)                                  â”‚
â”‚  â˜ TEST_REPORT.md signed by DevOps Lead                 â”‚
â”‚  â˜ TEST_REPORT.md signed by QA Lead                     â”‚
â”‚  â˜ Rollback plan tested and documented                  â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**If ANY checkbox is unchecked, deployment is REJECTED.**  
**å¦‚æœä»»ä½•è¤‡é¸æ¡†æœªå‹¾é¸ï¼Œéƒ¨ç½²å°‡è¢«æ‹’çµ•ã€‚**

### What Happens When Hard Gates Fail (ç¡¬æ€§é–˜é–€å¤±æ•—æ™‚æœƒç™¼ç”Ÿä»€éº¼)

**Immediate Action:**
- Pull request cannot be merged
- Code review will request changes
- CI/CD pipeline will fail

**ç«‹å³è¡Œå‹•ï¼š**
- æ‹‰å–è«‹æ±‚ç„¡æ³•åˆä½µ
- ç¨‹å¼ç¢¼å¯©æŸ¥å°‡è«‹æ±‚è®Šæ›´
- CI/CD æµç¨‹å°‡å¤±æ•—

**To Proceed:**
1. Fix the issues identified by automated checks
2. Push updated code
3. Re-run checks

**ç¹¼çºŒé€²è¡Œï¼š**
1. ä¿®å¾©è‡ªå‹•åŒ–æª¢æŸ¥è­˜åˆ¥çš„å•é¡Œ
2. æ¨é€æ›´æ–°çš„ç¨‹å¼ç¢¼
3. é‡æ–°é‹è¡Œæª¢æŸ¥

**Note**: Other organizational consequences (training, performance reviews, contract terms) are outside the scope of this technical framework and determined by management.  
**æ³¨æ„**ï¼šå…¶ä»–çµ„ç¹”å¾Œæœï¼ˆåŸ¹è¨“ã€ç¸¾æ•ˆè©•ä¼°ã€åˆç´„æ¢æ¬¾ï¼‰ä¸åœ¨æ­¤æŠ€è¡“æ¡†æ¶ç¯„åœå…§ï¼Œç”±ç®¡ç†å±¤æ±ºå®šã€‚

---

## For Internal Teams and Vendors (çµ¦å…§éƒ¨åœ˜éšŠèˆ‡å» å•†)

### Universal Standards (é€šç”¨æ¨™æº–)

**These standards apply equally to everyone. There are no special exceptions.**  
**é€™äº›æ¨™æº–å¹³ç­‰é©ç”¨æ–¼æ¯å€‹äººã€‚æ²’æœ‰ç‰¹æ®Šä¾‹å¤–ã€‚**

| Party | Compliance Requirement | Enforcement |
|-------|------------------------|-------------|
| **Jasslin Internal Engineers** | Full compliance with all standards | Performance reviews, career advancement tied to compliance |
| **Third-Party Vendors** | Full compliance with all standards | Contract renewal dependent on compliance record |
| **Senior Staff Engineers** | Full compliance + mentorship of junior staff | Leadership responsibility for team compliance |
| **Junior Developers** | Full compliance under supervision | Learning opportunity with guided reviews |

| å°è±¡ | åˆè¦è¦æ±‚ | åŸ·è¡Œ |
|-----|---------|-----|
| **Jasslin å…§éƒ¨å·¥ç¨‹å¸«** | å®Œå…¨éµå®ˆæ‰€æœ‰æ¨™æº– | ç¸¾æ•ˆè©•ä¼°ã€è·æ¥­æ™‰å‡èˆ‡åˆè¦æ€§æ›é‰¤ |
| **ç¬¬ä¸‰æ–¹å» å•†** | å®Œå…¨éµå®ˆæ‰€æœ‰æ¨™æº– | åˆç´„çºŒç´„å–æ±ºæ–¼åˆè¦è¨˜éŒ„ |
| **è³‡æ·±å·¥ç¨‹å¸«** | å®Œå…¨éµå®ˆ + æŒ‡å°åˆç´šå“¡å·¥ | åœ˜éšŠåˆè¦çš„é ˜å°è²¬ä»» |
| **åˆç´šé–‹ç™¼è€…** | åœ¨ç›£ç£ä¸‹å®Œå…¨éµå®ˆ | åœ¨æŒ‡å°å¯©æŸ¥ä¸‹çš„å­¸ç¿’æ©Ÿæœƒ |

### What Jasslin Provides (Jasslin æä¾›çš„æ”¯æ´)

To ensure successful compliance, Jasslin provides:  
ç‚ºç¢ºä¿æˆåŠŸåˆè¦ï¼ŒJasslin æä¾›ï¼š

âœ… **Complete documentation templates** in `/docs` for reference  
âœ… **å®Œæ•´çš„æ–‡ä»¶è¨˜éŒ„ç¯„æœ¬**åœ¨ `/docs` ä¸­ä¾›åƒè€ƒ

âœ… **Staging environments** that mirror production for testing  
âœ… **é ç™¼ç’°å¢ƒ**èˆ‡ç”Ÿç”¢ç’°å¢ƒç›¸åŒä»¥é€²è¡Œæ¸¬è©¦

âœ… **Documentation standards training materials** and certification program  
âœ… **æ–‡ä»¶æ¨™æº–åŸ¹è¨“ææ–™**å’Œèªè­‰è¨ˆåŠƒ

âœ… **Technical review support** for documentation and architecture  
âœ… **æŠ€è¡“å¯©æŸ¥æ”¯æ´**æ–‡ä»¶è¨˜éŒ„å’Œæ¶æ§‹

âœ… **Monitoring and alerting infrastructure** for early failure detection  
âœ… **ç›£æ§å’Œå‘Šè­¦åŸºç¤è¨­æ–½**ç”¨æ–¼æ—©æœŸæ•…éšœåµæ¸¬

âœ… **Post-incident support** for root cause analysis and remediation  
âœ… **äº‹æ•…å¾Œæ”¯æ´**æ ¹æœ¬åŸå› åˆ†æå’Œè£œæ•‘

### What Jasslin Will Not Accept (Jasslin ä¸æ¥å—çš„è¡Œç‚º)

âŒ **"Trust me, it works"** â€” Trust must be verified through testing  
âŒ **ã€Œç›¸ä¿¡æˆ‘ï¼Œå®ƒèƒ½é‹ä½œã€** â€” ä¿¡ä»»å¿…é ˆé€éæ¸¬è©¦é©—è­‰

âŒ **"I don't have time for documentation"** â€” Documentation is not optional  
âŒ **ã€Œæˆ‘æ²’æœ‰æ™‚é–“å¯«æ–‡ä»¶ã€** â€” æ–‡ä»¶è¨˜éŒ„ä¸æ˜¯å¯é¸çš„

âŒ **"This is how I've always done it"** â€” Past practices do not excuse current negligence  
âŒ **ã€Œæˆ‘ä¸€ç›´éƒ½æ˜¯é€™æ¨£åšçš„ã€** â€” éå»çš„åšæ³•ä¸èƒ½æˆç‚ºç•¶å‰ç–å¿½çš„è—‰å£

âŒ **"The reboot test is unnecessary"** â€” The incident proved this is false  
âŒ **ã€Œé‡å•Ÿæ¸¬è©¦æ˜¯ä¸å¿…è¦çš„ã€** â€” äº‹æ•…è­‰æ˜é€™æ˜¯éŒ¯èª¤çš„

âŒ **"I'll fix the documentation later"** â€” Documentation is part of the deliverable, not an afterthought  
âŒ **ã€Œæˆ‘ç¨å¾Œæœƒä¿®æ­£æ–‡ä»¶ã€** â€” æ–‡ä»¶è¨˜éŒ„æ˜¯äº¤ä»˜ç‰©çš„ä¸€éƒ¨åˆ†ï¼Œè€Œéäº‹å¾Œæƒ³æ³•

### Success Stories (æˆåŠŸæ¡ˆä¾‹)

**We celebrate and recognize teams that:**  
**æˆ‘å€‘æ…¶ç¥ä¸¦è¡¨å½°ä»¥ä¸‹åœ˜éšŠï¼š**

ğŸ† Achieve 100% documentation compliance on first submission  
ğŸ† é¦–æ¬¡æäº¤å³é”åˆ° 100% æ–‡ä»¶åˆè¦

ğŸ† Proactively improve documentation beyond minimum requirements  
ğŸ† ä¸»å‹•æ”¹é€²æ–‡ä»¶è¨˜éŒ„ï¼Œè¶…è¶Šæœ€ä½è¦æ±‚

ğŸ† Share lessons learned and best practices with other teams  
ğŸ† èˆ‡å…¶ä»–åœ˜éšŠåˆ†äº«ç¶“é©—æ•™è¨“å’Œæœ€ä½³å¯¦è¸

ğŸ† Identify and prevent potential failures through thorough testing  
ğŸ† é€šéå¾¹åº•æ¸¬è©¦è­˜åˆ¥ä¸¦é é˜²æ½›åœ¨æ•…éšœ

**Compliance is not a burden; it is a mark of professionalism.**  
**åˆè¦ä¸æ˜¯è² æ“”ï¼›å®ƒæ˜¯å°ˆæ¥­çš„æ¨™èªŒã€‚**

---

## Continuous Improvement (æŒçºŒæ”¹é€²)

### Documentation Review Cycle (æ–‡ä»¶å¯©æŸ¥é€±æœŸ)

All documentation MUST be reviewed and updated:  
æ‰€æœ‰æ–‡ä»¶è¨˜éŒ„å¿…é ˆå¯©æŸ¥å’Œæ›´æ–°ï¼š

| Trigger | Timeline | Required Actions |
|---------|----------|------------------|
| **After Production Incident** | Within 72 hours | Update RESILIENCE.md with new recovery SOP |
| **Quarterly Review** | Every 3 months | Verify all information is accurate and current |
| **Before Major Version Upgrade** | Prior to deployment | Update for dependency changes, new services |
| **After Infrastructure Changes** | Within 1 week | Update ARCHITECTURE.md and DEPLOY.md |

| è§¸ç™¼æ¢ä»¶ | æ™‚é–“è»¸ | å¿…éœ€è¡Œå‹• |
|---------|-------|---------|
| **ç”Ÿç”¢äº‹æ•…å¾Œ** | 72 å°æ™‚å…§ | ä½¿ç”¨æ–°çš„æ¢å¾© SOP æ›´æ–° RESILIENCE.md |
| **å­£åº¦å¯©æŸ¥** | æ¯ 3 å€‹æœˆ | é©—è­‰æ‰€æœ‰è³‡è¨Šæº–ç¢ºä¸”æœ€æ–° |
| **ä¸»è¦ç‰ˆæœ¬å‡ç´šå‰** | éƒ¨ç½²å‰ | æ›´æ–°ä¾è³´è®Šæ›´ã€æ–°æœå‹™ |
| **åŸºç¤è¨­æ–½è®Šæ›´å¾Œ** | 1 é€±å…§ | æ›´æ–° ARCHITECTURE.md å’Œ DEPLOY.md |

### Feedback Welcome (æ­¡è¿åé¥‹)

This framework is not static. We welcome constructive feedback:  
æ­¤æ¡†æ¶ä¸æ˜¯éœæ…‹çš„ã€‚æˆ‘å€‘æ­¡è¿å»ºè¨­æ€§åé¥‹ï¼š

ğŸ“§ Submit documentation improvement proposals  
ğŸ“§ æäº¤æ–‡ä»¶è¨˜éŒ„æ”¹é€²ææ¡ˆ

ğŸ› Report gaps or inaccuracies in templates  
ğŸ› å ±å‘Šç¯„æœ¬ä¸­çš„ç¼ºå£æˆ–ä¸æº–ç¢º

ğŸ’¡ Share lessons learned from deployments or incidents  
ğŸ’¡ åˆ†äº«å¾éƒ¨ç½²æˆ–äº‹æ•…ä¸­å­¸åˆ°çš„ç¶“é©—æ•™è¨“

ğŸ”§ Suggest new verification tests or automation tools  
ğŸ”§ å»ºè­°æ–°çš„é©—è­‰æ¸¬è©¦æˆ–è‡ªå‹•åŒ–å·¥å…·

**However**: Feedback does not grant exemption from current standards. All deployments must comply with the existing requirements while improvements are discussed.  
**ç„¶è€Œ**ï¼šåé¥‹ä¸æˆäºˆç•¶å‰æ¨™æº–çš„è±å…ã€‚åœ¨è¨è«–æ”¹é€²çš„åŒæ™‚ï¼Œæ‰€æœ‰éƒ¨ç½²éƒ½å¿…é ˆéµå®ˆç¾æœ‰çš„è¦æ±‚ã€‚

---

## Conclusion (çµè«–)

### This Is Not Bureaucracy. This Is Survival.
### é€™ä¸æ˜¯å®˜åƒšä¸»ç¾©ã€‚é€™æ˜¯ç”Ÿå­˜ä¹‹é“ã€‚

The incident that necessitated this documentation framework was **entirely preventable**. It was not caused by complex technical challenges or unforeseen circumstances. It was caused by:  
ä¿ƒæˆæ­¤æ–‡ä»¶æ¡†æ¶çš„äº‹æ•…æ˜¯**å®Œå…¨å¯é é˜²çš„**ã€‚å®ƒä¸æ˜¯ç”±è¤‡é›œçš„æŠ€è¡“æŒ‘æˆ°æˆ–ä¸å¯é è¦‹çš„æƒ…æ³é€ æˆçš„ã€‚å®ƒæ˜¯ç”±ä»¥ä¸‹åŸå› é€ æˆçš„ï¼š

- Basic professional failures  
  åŸºæœ¬çš„å°ˆæ¥­å¤±èª¤

- Reliance on human memory instead of written procedures  
  ä¾è³´äººé¡è¨˜æ†¶è€Œéæ›¸é¢ç¨‹åº

- Absence of verification and accountability  
  ç¼ºä¹é©—è­‰å’Œå•è²¬åˆ¶

**We lost client trust because we failed to meet basic professional standards.**  
**æˆ‘å€‘å¤±å»äº†å®¢æˆ¶ä¿¡ä»»ï¼Œå› ç‚ºæˆ‘å€‘æœªèƒ½é”åˆ°åŸºæœ¬çš„å°ˆæ¥­æ¨™æº–ã€‚**

**This framework exists to ensure this never happens again.**  
**æ­¤æ¡†æ¶çš„å­˜åœ¨æ˜¯ç‚ºäº†ç¢ºä¿é€™æ°¸ä¸å†ç™¼ç”Ÿã€‚**

---

### Our Commitment (æˆ‘å€‘çš„æ‰¿è«¾)

By adhering to these standards, we guarantee:  
é€šééµå®ˆé€™äº›æ¨™æº–ï¼Œæˆ‘å€‘ä¿è­‰ï¼š

âœ… **100% Service Resilience** â€” All systems survive failures and reboot automatically  
âœ… **100% æœå‹™éŸŒæ€§** â€” æ‰€æœ‰ç³»çµ±ç¶“å¾—èµ·æ•…éšœä¸¦è‡ªå‹•é‡å•Ÿ

âœ… **100% Reproducibility** â€” Any engineer can deploy using only documentation  
âœ… **100% å¯é‡ç¾æ€§** â€” ä»»ä½•å·¥ç¨‹å¸«åƒ…ä½¿ç”¨æ–‡ä»¶å³å¯éƒ¨ç½²

âœ… **100% Accountability** â€” Clear standards, clear consequences, clear records  
âœ… **100% å•è²¬åˆ¶** â€” æ˜ç¢ºçš„æ¨™æº–ã€æ˜ç¢ºçš„å¾Œæœã€æ˜ç¢ºçš„è¨˜éŒ„

âœ… **Zero Tolerance for Negligence** â€” Professional engineering is not optional  
âœ… **å°ç–å¿½é›¶å®¹å¿** â€” å°ˆæ¥­å·¥ç¨‹ä¸æ˜¯å¯é¸çš„

---

### Final Message (æœ€å¾Œè¨Šæ¯)

**To all engineers, vendors, and partners working on Jasslin-managed systems:**  
**è‡´æ‰€æœ‰åœ¨ Jasslin è¨—ç®¡ç³»çµ±ä¸Šå·¥ä½œçš„å·¥ç¨‹å¸«ã€å» å•†å’Œåˆä½œå¤¥ä¼´ï¼š**

You are not just writing code. You are building systems that our clients depend on to run their businesses.  
æ‚¨ä¸åƒ…åƒ…æ˜¯åœ¨ç·¨å¯«ç¨‹å¼ç¢¼ã€‚æ‚¨æ­£åœ¨æ§‹å»ºæˆ‘å€‘çš„å®¢æˆ¶ä¾è³´æ–¼é‹ç‡Ÿå…¶æ¥­å‹™çš„ç³»çµ±ã€‚

When you skip documentation, you endanger client operations.  
ç•¶æ‚¨è·³éæ–‡ä»¶è¨˜éŒ„æ™‚ï¼Œæ‚¨å±åŠå®¢æˆ¶æ¥­å‹™é‹ç‡Ÿã€‚

When you skip the reboot test, you introduce hidden vulnerabilities.  
ç•¶æ‚¨è·³éé‡å•Ÿæ¸¬è©¦æ™‚ï¼Œæ‚¨å¼•å…¥éš±è—çš„æ¼æ´ã€‚

When you bypass staging, you gamble with production stability.  
ç•¶æ‚¨ç¹éé ç™¼ç’°å¢ƒæ™‚ï¼Œæ‚¨ä»¥ç”Ÿç”¢ç©©å®šæ€§ç‚ºè³­æ³¨ã€‚

**Professional engineering is not about speed. It is about reliability.**  
**å°ˆæ¥­å·¥ç¨‹ä¸æ˜¯é—œæ–¼é€Ÿåº¦ã€‚å®ƒæ˜¯é—œæ–¼å¯é æ€§ã€‚**

**This framework is not a suggestion. It is mandatory.**  
**æ­¤æ¡†æ¶ä¸æ˜¯å»ºè­°ã€‚å®ƒæ˜¯å¼·åˆ¶æ€§çš„ã€‚**

**This is how we protect our clients.**  
**é€™æ˜¯æˆ‘å€‘ä¿è­·å®¢æˆ¶çš„æ–¹å¼ã€‚**

**This is how we protect Jasslin's reputation.**  
**é€™æ˜¯æˆ‘å€‘ä¿è­· Jasslin è²è­½çš„æ–¹å¼ã€‚**

**This is how we protect our professional integrity.**  
**é€™æ˜¯æˆ‘å€‘ä¿è­·å°ˆæ¥­èª ä¿¡çš„æ–¹å¼ã€‚**

---

## Authorization (æˆæ¬Šç°½ç½²)

This document establishes the **Production Service Documentation Standards** as the mandatory framework for all Jasslin-managed services.  
æœ¬æ–‡ä»¶å»ºç«‹**ç”Ÿç”¢æœå‹™æ–‡ä»¶æ¨™æº–**ä½œç‚ºæ‰€æœ‰ Jasslin è¨—ç®¡æœå‹™çš„å¼·åˆ¶æ€§æ¡†æ¶ã€‚

This framework is **effective immediately** and supersedes all previous informal practices, verbal agreements, or undocumented procedures.  
æ­¤æ¡†æ¶**ç«‹å³ç”Ÿæ•ˆ**ï¼Œä¸¦å–ä»£æ‰€æœ‰å…ˆå‰çš„éæ­£å¼åšæ³•ã€å£é ­å”è­°æˆ–æœªè¨˜éŒ„çš„ç¨‹åºã€‚

**Authorized By (æˆæ¬Šäºº):**  

**Epaphras Wu**  
**å³è±å‰**  
Engineer, Jasslin  
å·¥ç¨‹å¸«ï¼ŒJasslin

---

**Effective Date (ç”Ÿæ•ˆæ—¥æœŸ):**  
2026-02-02

**Document Version (æ–‡ä»¶ç‰ˆæœ¬):**  
1.0

**Next Mandatory Review (ä¸‹æ¬¡å¼·åˆ¶å¯©æŸ¥):**  
2026-05-02

---

**For questions, clarifications, or compliance support:**  
**å¦‚æœ‰å•é¡Œã€éœ€è¦æ¾„æ¸…æˆ–åˆè¦æ”¯æ´ï¼š**

Contact: Jasslin Engineering Team  
è¯ç¹«ï¼šJasslin å·¥ç¨‹åœ˜éšŠ

---

> **Remember (è«‹è¨˜ä½):**
>
> **Professional engineering saves businesses, protects reputations, and builds trust.**  
> **å°ˆæ¥­å·¥ç¨‹æ‹¯æ•‘æ¥­å‹™ã€ä¿è­·è²è­½ä¸¦å»ºç«‹ä¿¡ä»»ã€‚**
>
> **These standards are not obstacles. They are the foundation of excellence.**  
> **é€™äº›æ¨™æº–ä¸æ˜¯éšœç¤™ã€‚å®ƒå€‘æ˜¯å“è¶Šçš„åŸºç¤ã€‚**
