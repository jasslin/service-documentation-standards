# Production Service Documentation Standards
# ç”Ÿç”¢æœå‹™æ–‡æª”æ¨™æº–

**Repository Purpose**: Central governance for Jasslin managed services  
**å„²å­˜åº«ç›®çš„**ï¼šJasslin è¨—ç®¡æœå‹™çš„ä¸­å¤®æ²»ç†

**Version**: 2.0.0  
**Last Updated**: 2026-02-02

---

## Quick Navigation (å¿«é€Ÿå°èˆª)

| Document | Purpose | Audience |
|----------|---------|----------|
| **[RELEASE_POLICY.md](RELEASE_POLICY.md)** | Production release requirements, mandatory gates | Engineers deploying to production |
| **[OPS_RUNBOOK.md](OPS_RUNBOOK.md)** | Index of all service documentation locations | On-call engineers, operations |
| **[templates/docs/](templates/docs/)** | Documentation templates (ARCHITECTURE, DEPLOY, etc.) | Engineers creating new services |

---

## The Incident (äº‹æ•…èµ·æº)

### What Happened (ç™¼ç”Ÿäº†ä»€éº¼)

**Date**: January 2026  
**Service**: Flemabus (mission-critical client system)  
**Total Downtime**: **TWO WEEKS**

A routine server reboot triggered complete service failure. Multiple engineers attempted recovery but failed. The system was eventually restored only after consulting a specific engineer who held critical knowledge in memory.

### Root Causes (æ ¹æœ¬åŸå› )

The investigation revealed **five critical failures**:

1. **Network Naming Conflicts**  
   New deployment used generic network names (app-network), conflicting with production, breaking BOTH systems.

2. **Accidental `docker-compose down`**  
   Engineer ran command in wrong directory, bringing down production for 2 weeks.

3. **No Rollback Capability**  
   No git tags, no backup configuration, no way to restore previous working state.

4. **Service Persistence Not Configured**  
   Docker daemon not auto-enabled, no `restart: always` policies.

5. **Knowledge Single-Point-of-Failure**  
   Critical system knowledge existed only in one person's memory. Multiple engineers couldn't recover the system.

### Business Impact (æ¥­å‹™å½±éŸ¿)

- **2 weeks** of zero service availability
- Complete client business interruption
- Major SLA violation penalties
- Contract termination risk
- Severe reputational damage
- Potential legal liability

### The Lesson (æ•™è¨“)

**This incident was 100% preventable.**

It was NOT caused by complex technical challenges or unforeseen edge cases.

It WAS caused by:
- Lack of environment isolation (network conflicts)
- Manual operations without safeguards (wrong-directory accidents)
- No versioning or rollback plan
- Missing service persistence configuration
- No documentation (knowledge concentration)

---

## The Solution: Technical Controls (è§£æ±ºæ–¹æ¡ˆï¼šæŠ€è¡“æ§åˆ¶)

This repository defines **enforceable technical requirements** that prevent these failures.

### ğŸ”´ Hard Gates (Mandatory, Blocks Release)

Eight automated checks that **MUST pass** before any production deployment:

1. **Merge Control** ğŸ”´ â€” Branch protection + CODEOWNERS + CI (enforcement mechanism)
2. **Automated Release** ğŸ”´ â€” Pipeline-only deployment, no manual SSH (prevents accidental docker-compose down)
3. **Least Privilege** ğŸ”´ â€” Read-only vendor access (limits blast radius, risk management not trust)
4. **Environment Isolation** â€” Project-specific naming (prevents network conflicts)
5. **Git-Tracked Configuration** â€” No manual operations (prevents wrong-directory accidents)
6. **Rollback Capability** â€” Git tags + deployment snapshots (30-second rollback vs 2-week recovery)
7. **Service Persistence** â€” restart: always + healthcheck (survives reboots)
8. **Documentation** â€” 4 required files (eliminates knowledge single-point-of-failure)

**Gates #1-3 are enforcement mechanisms. Gates #4-8 are validated requirements.**  
**No need to ask for transparency â€” technical controls enforce it automatically.**

**Details**: See [RELEASE_POLICY.md](RELEASE_POLICY.md)  
**Setup Guide**: See [SETUP_BRANCH_PROTECTION.md](SETUP_BRANCH_PROTECTION.md)

### ğŸŸ¡ Aspirational Standards (Recommended)

Best practices that improve quality but don't block deployment:
- Unit test coverage >80%
- Bilingual documentation (English + Chinese)
- Hard reboot test on staging
- Performance benchmarks
- QA sign-off

---

## For Engineers Deploying to Production

### Before Every Release:

```bash
# 1. Run validation
bash scripts/validate-hardgates.sh

# 2. If all pass â†’ Submit PR
# 3. After approval â†’ Tag and deploy
git tag -a v1.2.3 -m "Production release"
git push origin v1.2.3

# 4. On production server
cd /opt/[service-name]
git fetch --tags
git checkout v1.2.3
docker-compose up -d
```

**Full procedures**: [RELEASE_POLICY.md](RELEASE_POLICY.md)

---

## For On-Call Engineers

### When Service is Down:

1. **Find service documentation**  
   â†’ See [OPS_RUNBOOK.md](OPS_RUNBOOK.md) for locations

2. **Read RESILIENCE.md** (in service's repository)  
   â†’ Recovery procedures and rollback steps

3. **Attempt rollback** (if recent deployment):
   ```bash
   cd /opt/[service-name]
   git checkout v[previous-version]
   docker-compose up -d
   ```

4. **If recovery fails**  
   â†’ Contact service owner (listed in OPS_RUNBOOK.md)

**Full index**: [OPS_RUNBOOK.md](OPS_RUNBOOK.md)

---

## For Engineers Creating New Services

### Required Documentation (4 files):

| Template | Purpose |
|----------|---------|
| **ARCHITECTURE.md** | System design, Mermaid diagram, dependencies |
| **DEPLOY.md** | Deployment steps, environment variables |
| **RESILIENCE.md** | Self-healing config, recovery procedures |
| **TEST_REPORT.md** | Staging verification, hard reboot test |

**Templates available**: [templates/docs/](templates/docs/)

### Setup:

```bash
# 1. Copy templates to your service repo
cp -r /path/to/documentation-management/templates/docs/ ./docs/

# 2. Fill in all 4 documents

# 3. Ensure docker-compose.yml meets requirements:
services:
  api:
    container_name: projectname-api  # Project-specific name
    restart: always                   # Required
    networks:
      - projectname-network           # Custom network
    healthcheck:                      # Required
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s

networks:
  projectname-network:
    driver: bridge

# 4. Validate
bash /path/to/documentation-management/scripts/validate-hardgates.sh

# 5. Tag first production release
git tag -a v1.0.0 -m "Initial production release"
```

---

## Repository Structure (å„²å­˜åº«çµæ§‹)

```
documentation-management/
â”œâ”€â”€ README.md                         â† You are here
â”œâ”€â”€ RELEASE_POLICY.md                â† Production release requirements (all 8 gates)
â”œâ”€â”€ OPS_RUNBOOK.md                   â† Index to all service documentation
â”œâ”€â”€ SETUP_BRANCH_PROTECTION.md       â† Gate #1: GitHub branch protection
â”œâ”€â”€ SETUP_SSH_RESTRICTION.md         â† Gate #2: SSH access restriction
â”œâ”€â”€ SETUP_LEAST_PRIVILEGE.md         â† Gate #3: Least privilege access
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ validate-hardgates.sh        â† Automated validation (used by CI)
â”‚   â””â”€â”€ snapshot-release.sh          â† Release snapshot generator
â””â”€â”€ templates/
    â”œâ”€â”€ CODEOWNERS                    â† Code review enforcement
    â”œâ”€â”€ github-workflow-validate.yml  â† CI validation workflow
    â”œâ”€â”€ github-workflow-deploy.yml    â† Production deployment pipeline
    â””â”€â”€ docs/                          â† Documentation templates
        â”œâ”€â”€ ARCHITECTURE.md
        â”œâ”€â”€ DEPLOY.md
        â”œâ”€â”€ RESILIENCE.md
        â””â”€â”€ TEST_REPORT.md
```

---

## Policy Enforcement (æ”¿ç­–åŸ·è¡Œ)

### How Requirements are Enforced:

1. **Automated validation** â€” `validate-hardgates.sh` runs on every PR
2. **CI/CD integration** â€” Cannot merge if validation fails
3. **Git tag requirement** â€” Production servers only accept tagged versions
4. **PR approval required** â€” No direct commits to main branch

**These are technical controls, not trust-based policies.**  
**é€™äº›æ˜¯æŠ€è¡“æ§åˆ¶ï¼Œè€ŒéåŸºæ–¼ä¿¡ä»»çš„æ”¿ç­–ã€‚**

### What Happens When Gates Fail:

- CI/CD pipeline fails (red X on PR)
- Cannot merge until fixed
- Deployment blocked

**No exceptions. No overrides** (except critical production outage with Engineering Lead approval).

---

## Scope & Authority (ç¯„åœèˆ‡æ¬Šé™)

### What This Repository Governs:

- âœ… Technical requirements for production deployment
- âœ… Documentation standards
- âœ… Automated validation criteria
- âœ… Rollback procedures

### What This Repository Does NOT Govern:

- âŒ Code quality standards (that's in individual repos)
- âŒ Business logic requirements
- âŒ HR/contractual enforcement
- âŒ QA approval processes (aspirational, not blocking)

### Authority:

**Authorized By**: Ezra Wu (å³è±å‰), Engineer  
**Scope**: Technical deployment requirements enforceable by engineers  
**Enforcement**: Automated CI/CD checks

---

## Version History (ç‰ˆæœ¬æ­·å²)

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 2.0.0 | 2026-02-02 | Restructure: separate RELEASE_POLICY and OPS_RUNBOOK | Ezra Wu |
| 1.0.0 | 2026-02-01 | Initial framework based on Flemabus incident | Ezra Wu |

---

## Questions & Contact (å•é¡Œèˆ‡è¯çµ¡)

### For Deployment Questions:
â†’ Read [RELEASE_POLICY.md](RELEASE_POLICY.md)

### For Operational Issues:
â†’ Read [OPS_RUNBOOK.md](OPS_RUNBOOK.md)

### For Policy Changes:
â†’ Submit PR to this repository  
â†’ Contact: Engineering Lead

---

**"Documentation is not bureaucracy. It is the foundation of reliability."**  
**ã€Œæ–‡ä»¶è¨˜éŒ„ä¸æ˜¯å®˜åƒšä¸»ç¾©ã€‚å®ƒæ˜¯å¯é æ€§çš„åŸºç¤ã€‚ã€**
